{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Brief History of Time - Stephen Hawking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Lobraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392175\n",
      "A BRIEF HISTORY OF TIME\n",
      "\n",
      "About the Book\n",
      "Was there a beginning of time? Could time run backwards? \n"
     ]
    }
   ],
   "source": [
    "# Open the text file\n",
    "with open('./History of Time.txt', 'rb') as f:\n",
    "    text = f.read().decode(encoding='utf-8')\n",
    "print(len(text))\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "'A BRIEF HISTORY OF T' mapped to [26  2 27 43 34 30 31  2 33 34 44 45 40 43 50  2 40 31  2 45]\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(text))) # vocabulary\n",
    "vocab_size = len(vocab) # len of the vocabulary\n",
    "char2id = {u:i for i,u in enumerate(vocab)} # mapping the character from the text to a specific id\n",
    "id2char = np.array(vocab)\n",
    "text_as_ip = np.array([char2id[c] for c in text]) # storing characters as their ids\n",
    "print(vocab_size)\n",
    "print(f\"'{text[:20]}' mapped to {text_as_ip[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SEQ_LEN = 100\n",
    "example_per_epoch = len(text)//(SEQ_LEN+1)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and shaping the data\n",
    "x_ip = []\n",
    "y_op = []\n",
    "for i in range(len(text_as_ip) - SEQ_LEN):\n",
    "    x_ip.append(text_as_ip[i:SEQ_LEN+i])\n",
    "    y_op.append(text_as_ip[i+1:SEQ_LEN+i+1])\n",
    "x_ip = np.array(x_ip)\n",
    "y_op = np.array(y_op)\n",
    "x_ip, _, y_op, _ = train_test_split(x_ip, y_op, shuffle=True, test_size=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into labels and target\n",
    "def split_ip_target(ip, op, batch_size):\n",
    "    data = []\n",
    "    label = []\n",
    "    for i in range(0, len(ip) - batch_size, batch_size):\n",
    "        data.append(ip[i : batch_size+i])\n",
    "        label.append(op[i : batch_size+i])\n",
    "    return np.array(data), np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = split_ip_target(x_ip, y_op, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trnx, testx, trny, testy = train_test_split(data, label, shuffle=True, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 86)\n"
     ]
    }
   ],
   "source": [
    "pred = model(trnx[1])\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           22016     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (64, None, 1024)          6297600   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 256)           262400    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (64, None, 256)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (64, None, 128)           32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, None, 86)            11094     \n",
      "=================================================================\n",
      "Total params: 10,564,310\n",
      "Trainable params: 10,564,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48, 16, 70, 62,  8, 48,  9,  6,  9,  7, 14, 53, 43, 44, 75, 37, 70,\n",
       "       61, 30, 17, 72, 15, 13, 82, 54, 24, 25, 81,  7, 62, 75, 53, 26, 74,\n",
       "        7, 71, 31, 24, 28, 57, 50, 75, 54, 17, 62, 27, 22,  8, 34,  9, 35,\n",
       "       19, 27, 30, 55, 71, 76, 59, 58, 67, 76, 11, 56, 22, 46, 59, 72, 36,\n",
       "       31, 49,  2, 79, 23, 79,  9, 16,  8, 16, 15, 47, 61, 41, 68,  3, 39,\n",
       "       25, 47, 75, 80, 56, 25, 70, 33, 10, 41, 70, 85, 80, 24, 44],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure that splitting works fine\n",
    "sample = tf.random.categorical(pred[0], num_samples=1)\n",
    "sample = tf.squeeze(sample,axis=-1).numpy()\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4sk,W-)-+2bRSxLsjE5u31–c=?ö+kxbAw+tF=CfYxc5kB:,I-J7BEdtyhgpy/e:UhuKFX é;é-4,43VjPq!N?Vxëe?sH.Ps−ë=S\n"
     ]
    }
   ],
   "source": [
    "print(''.join(id2char[sample]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4537053\n"
     ]
    }
   ],
   "source": [
    "loss_fn = lambda labels, logits: tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "ex_loss = loss_fn(trny[1], pred)\n",
    "print(ex_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function \n",
    "def train_step(ip, op):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(ip)\n",
    "        loss = loss_fn(op, pred)\n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch: 1 => Batch: 0, Loss: 4.453649997711182\n",
      "At epoch: 1 => Batch: 100, Loss: 2.315669059753418\n",
      "At epoch: 1 => Batch: 200, Loss: 1.8315255641937256\n",
      "At epoch: 1 => Batch: 300, Loss: 1.4907742738723755\n",
      "At epoch: 1 => Batch: 400, Loss: 1.2461707592010498\n",
      "At epoch: 1 => Batch: 500, Loss: 1.1214592456817627\n",
      "At epoch: 1 => Batch: 600, Loss: 1.0999459028244019\n",
      "At epoch: 1 => Batch: 700, Loss: 1.0299654006958008\n",
      "At epoch: 1 => Batch: 800, Loss: 0.970791757106781\n",
      "At epoch: 1 => Batch: 900, Loss: 0.91861891746521\n",
      "At epoch: 1 => Batch: 1000, Loss: 0.903546154499054\n",
      "At epoch: 1 => Batch: 1100, Loss: 0.8604124188423157\n",
      "At epoch: 1 => Batch: 1200, Loss: 0.7837527394294739\n",
      "At epoch: 1 => Batch: 1300, Loss: 0.7501669526100159\n",
      "At epoch: 1 => Batch: 1400, Loss: 0.7065208554267883\n",
      "At epoch: 1 => Batch: 1500, Loss: 0.6665033102035522\n",
      "At epoch: 1 => Batch: 1600, Loss: 0.652634859085083\n",
      "At epoch: 1 => Batch: 1700, Loss: 0.581480860710144\n",
      "At epoch: 1 => Batch: 1800, Loss: 0.5469928979873657\n",
      "At epoch: 1 => Batch: 1900, Loss: 0.508774995803833\n",
      "At epoch: 1 => Batch: 2000, Loss: 0.468321293592453\n",
      "At epoch: 1 => Batch: 2100, Loss: 0.433561235666275\n",
      "At epoch: 1 => Batch: 2200, Loss: 0.4418776035308838\n",
      "At epoch: 1 => Batch: 2300, Loss: 0.41061797738075256\n",
      "At epoch: 1 => Batch: 2400, Loss: 0.3846425712108612\n",
      "At epoch: 1 => Batch: 2500, Loss: 0.38416099548339844\n",
      "At epoch: 1 => Batch: 2600, Loss: 0.3842187225818634\n",
      "At epoch: 1 => Batch: 2700, Loss: 0.33692315220832825\n",
      "At epoch: 1 => Batch: 2800, Loss: 0.3378356993198395\n",
      "At epoch: 1 => Batch: 2900, Loss: 0.3331909477710724\n",
      "At epoch: 1 => Batch: 3000, Loss: 0.33653536438941956\n",
      "At epoch: 1 => Batch: 3100, Loss: 0.3134722113609314\n",
      "At epoch: 1 => Batch: 3200, Loss: 0.30980661511421204\n",
      "At epoch: 1 => Batch: 3300, Loss: 0.3157780170440674\n",
      "At epoch: 1 => Batch: 3400, Loss: 0.33080941438674927\n",
      "At epoch: 1 => Batch: 3500, Loss: 0.3126934766769409\n",
      "At epoch: 1 => Batch: 3600, Loss: 0.3240947723388672\n",
      "At epoch: 1 => Batch: 3700, Loss: 0.3030417561531067\n",
      "At epoch: 1 => Batch: 3800, Loss: 0.2882055938243866\n",
      "At epoch: 1 => Batch: 3900, Loss: 0.29695743322372437\n",
      "At epoch: 1 => Batch: 4000, Loss: 0.3046378791332245\n",
      "At epoch: 1 => Batch: 4100, Loss: 0.30566415190696716\n",
      "At epoch: 1 => Batch: 4200, Loss: 0.2981213331222534\n",
      "At epoch: 2 => Batch: 0, Loss: 0.27232420444488525\n",
      "At epoch: 2 => Batch: 100, Loss: 0.30183687806129456\n",
      "At epoch: 2 => Batch: 200, Loss: 0.3063066303730011\n",
      "At epoch: 2 => Batch: 300, Loss: 0.30109310150146484\n",
      "At epoch: 2 => Batch: 400, Loss: 0.29959169030189514\n",
      "At epoch: 2 => Batch: 500, Loss: 0.28397536277770996\n",
      "At epoch: 2 => Batch: 600, Loss: 0.28601160645484924\n",
      "At epoch: 2 => Batch: 700, Loss: 0.29359930753707886\n",
      "At epoch: 2 => Batch: 800, Loss: 0.2794610857963562\n",
      "At epoch: 2 => Batch: 900, Loss: 0.28662005066871643\n",
      "At epoch: 2 => Batch: 1000, Loss: 0.29272735118865967\n",
      "At epoch: 2 => Batch: 1100, Loss: 0.2726942002773285\n",
      "At epoch: 2 => Batch: 1200, Loss: 0.28609347343444824\n",
      "At epoch: 2 => Batch: 1300, Loss: 0.27235186100006104\n",
      "At epoch: 2 => Batch: 1400, Loss: 0.26166021823883057\n",
      "At epoch: 2 => Batch: 1500, Loss: 0.2638012170791626\n",
      "At epoch: 2 => Batch: 1600, Loss: 0.29625874757766724\n",
      "At epoch: 2 => Batch: 1700, Loss: 0.281619668006897\n",
      "At epoch: 2 => Batch: 1800, Loss: 0.26168158650398254\n",
      "At epoch: 2 => Batch: 1900, Loss: 0.2639848291873932\n",
      "At epoch: 2 => Batch: 2000, Loss: 0.28600549697875977\n",
      "At epoch: 2 => Batch: 2100, Loss: 0.2720605731010437\n",
      "At epoch: 2 => Batch: 2200, Loss: 0.27968829870224\n",
      "At epoch: 2 => Batch: 2300, Loss: 0.286222368478775\n",
      "At epoch: 2 => Batch: 2400, Loss: 0.2755352854728699\n",
      "At epoch: 2 => Batch: 2500, Loss: 0.272931843996048\n",
      "At epoch: 2 => Batch: 2600, Loss: 0.2732211947441101\n",
      "At epoch: 2 => Batch: 2700, Loss: 0.26836201548576355\n",
      "At epoch: 2 => Batch: 2800, Loss: 0.2668769955635071\n",
      "At epoch: 2 => Batch: 2900, Loss: 0.27064889669418335\n",
      "At epoch: 2 => Batch: 3000, Loss: 0.27185115218162537\n",
      "At epoch: 2 => Batch: 3100, Loss: 0.27601420879364014\n",
      "At epoch: 2 => Batch: 3200, Loss: 0.2606610357761383\n",
      "At epoch: 2 => Batch: 3300, Loss: 0.2780662178993225\n",
      "At epoch: 2 => Batch: 3400, Loss: 0.277018278837204\n",
      "At epoch: 2 => Batch: 3500, Loss: 0.2779705822467804\n",
      "At epoch: 2 => Batch: 3600, Loss: 0.27200397849082947\n",
      "At epoch: 2 => Batch: 3700, Loss: 0.2638634741306305\n",
      "At epoch: 2 => Batch: 3800, Loss: 0.2681238651275635\n",
      "At epoch: 2 => Batch: 3900, Loss: 0.2752797305583954\n",
      "At epoch: 2 => Batch: 4000, Loss: 0.28169623017311096\n",
      "At epoch: 2 => Batch: 4100, Loss: 0.27564436197280884\n",
      "At epoch: 2 => Batch: 4200, Loss: 0.2778695225715637\n",
      "Wall time: 29min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "min_loss = np.inf\n",
    "EPOCHS = 2\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range(trnx.shape[0]):\n",
    "        ip = trnx[i]\n",
    "        op = trny[i]\n",
    "        loss = train_step(ip, op)\n",
    "        if (i%100==0):\n",
    "            mean_loss = loss.numpy().mean()\n",
    "            if mean_loss < min_loss:\n",
    "                min_loss = mean_loss\n",
    "                model.save_weights(\"./time/ckpt\")\n",
    "            print(f\"At epoch: {epoch+1} => Batch: {i}, Loss: {mean_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2798250615596771\n"
     ]
    }
   ],
   "source": [
    "test_loss = []\n",
    "for i,j in zip(testx, testy):\n",
    "    test_loss.append(loss_fn(j, model(i)).numpy().mean())\n",
    "\n",
    "print(f\"Test Loss: {np.mean(test_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            22016     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (1, None, 1024)           6297600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, None, 256)            262400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (1, None, 256)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (1, None, 128)            32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (1, None, 86)             11094     \n",
      "=================================================================\n",
      "Total params: 10,564,310\n",
      "Trainable params: 10,564,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_gen = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model_gen.load_weights(\"./time/ckpt\")\n",
    "model_gen.build(tf.TensorShape([1, None]))\n",
    "model_gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(mdl, start_str, temp, num_gen):\n",
    "    num_gen = num_gen\n",
    "    ip_eval = [char2id[s] for s in start_str]\n",
    "    ip_eval = tf.expand_dims(ip_eval, 0)\n",
    "    \n",
    "    text_gen = []\n",
    "    temp = temp\n",
    "    mdl.reset_states()\n",
    "    for i in range(num_gen):\n",
    "        pred = mdl(ip_eval)\n",
    "        pred = tf.squeeze(pred, 0)\n",
    "        pred = pred/temp\n",
    "        pred_id = tf.random.categorical(pred,num_samples=1)[-1, 0].numpy()\n",
    "        ip_eval = tf.expand_dims([pred_id], 0)\n",
    "        text_gen.append(id2char[pred_id])\n",
    "    \n",
    "    return (start_str + \"\".join(text_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time that happened to pame observe. Whatever the repulsive forces should space expanding at nearly the same rate in all different directions at a time when the density of the universe has its present value. In this case there is no unique stanowever, we still use Newton’s theory for all practical purposes because the difference between its predictions and those of general relativity is very small in the situations that we normally deal with. (Newton’s theory also has the great advantage that it left lots of room on a large scale, the microwave oven awa complete revolution, which had also to the sun, we would not be able to tell whether the leading authority on the structure of stars, predicts the laws that the universe should have begun in just this way, except as the act of a God who intended to create beings like us. In an attempt to find a model of the universe in which the collapsing phase looked like the time reverse of the expanding phase. The contracting phase will be unsuitable bec\n"
     ]
    }
   ],
   "source": [
    "# 'temp' parameter is used to create randomness; higher the temp, greater are the unfamiliar words.\n",
    "gen_text_time = gen_text(model_gen, start_str=\"Time\", temp=0.5, num_gen=1000)\n",
    "print(gen_text_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
